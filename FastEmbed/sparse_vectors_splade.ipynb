{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/qdrant_study/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fastembed import SparseTextEmbedding, SparseEmbedding\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'prithivida/Splade_PP_en_v1',\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1', 'url': None},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Independent Implementation of SPLADE++ Model for English.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.532,\n",
       "  'additional_files': [],\n",
       "  'requires_idf': None,\n",
       "  'vocab_size': 30522},\n",
       " {'model': 'prithvida/Splade_PP_en_v1',\n",
       "  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1', 'url': None},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Independent Implementation of SPLADE++ Model for English.',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.532,\n",
       "  'additional_files': [],\n",
       "  'requires_idf': None,\n",
       "  'vocab_size': 30522},\n",
       " {'model': 'Qdrant/bm42-all-minilm-l6-v2-attentions',\n",
       "  'sources': {'hf': 'Qdrant/all_miniLM_L6_v2_with_attentions', 'url': None},\n",
       "  'model_file': 'model.onnx',\n",
       "  'description': 'Light sparse embedding model, which assigns an importance score to each token in the text',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.09,\n",
       "  'additional_files': ['stopwords.txt'],\n",
       "  'requires_idf': True,\n",
       "  'vocab_size': 30522},\n",
       " {'model': 'Qdrant/bm25',\n",
       "  'sources': {'hf': 'Qdrant/bm25', 'url': None},\n",
       "  'model_file': 'mock.file',\n",
       "  'description': 'BM25 as sparse embeddings meant to be used with Qdrant',\n",
       "  'license': 'apache-2.0',\n",
       "  'size_in_GB': 0.01,\n",
       "  'additional_files': ['arabic.txt',\n",
       "   'azerbaijani.txt',\n",
       "   'basque.txt',\n",
       "   'bengali.txt',\n",
       "   'catalan.txt',\n",
       "   'chinese.txt',\n",
       "   'danish.txt',\n",
       "   'dutch.txt',\n",
       "   'english.txt',\n",
       "   'finnish.txt',\n",
       "   'french.txt',\n",
       "   'german.txt',\n",
       "   'greek.txt',\n",
       "   'hebrew.txt',\n",
       "   'hinglish.txt',\n",
       "   'hungarian.txt',\n",
       "   'indonesian.txt',\n",
       "   'italian.txt',\n",
       "   'kazakh.txt',\n",
       "   'nepali.txt',\n",
       "   'norwegian.txt',\n",
       "   'portuguese.txt',\n",
       "   'romanian.txt',\n",
       "   'russian.txt',\n",
       "   'slovene.txt',\n",
       "   'spanish.txt',\n",
       "   'swedish.txt',\n",
       "   'tajik.txt',\n",
       "   'turkish.txt'],\n",
       "  'requires_idf': True,\n",
       "  'vocab_size': 0}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparseTextEmbedding.list_supported_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44958/2946367021.py:3: DeprecationWarning: The right spelling is prithivida/Splade_PP_en_v1. Support of this name will be removed soon, please fix the model_name\n",
      "  model = SparseTextEmbedding(model_name=model_name)\n",
      "Fetching 5 files: 100%|██████████| 5/5 [01:28<00:00, 17.66s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"prithvida/Splade_PP_en_v1\"\n",
    "# This triggers the model download\n",
    "model = SparseTextEmbedding(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents: List[str] = [\n",
    "    \"Chandrayaan-3 is India's third lunar mission\",\n",
    "    \"It aimed to land a rover on the Moon's surface - joining the US, China and Russia\",\n",
    "    \"The mission is a follow-up to Chandrayaan-2, which had partial success\",\n",
    "    \"Chandrayaan-3 will be launched by the Indian Space Research Organisation (ISRO)\",\n",
    "    \"The estimated cost of the mission is around $35 million\",\n",
    "    \"It will carry instruments to study the lunar surface and atmosphere\",\n",
    "    \"Chandrayaan-3 landed on the Moon's surface on 23rd August 2023\",\n",
    "    \"It consists of a lander named Vikram and a rover named Pragyan similar to Chandrayaan-2. Its propulsion module would act like an orbiter.\",\n",
    "    \"The propulsion module carries the lander and rover configuration until the spacecraft is in a 100-kilometre (62 mi) lunar orbit\",\n",
    "    \"The mission used GSLV Mk III rocket for its launch\",\n",
    "    \"Chandrayaan-3 was launched from the Satish Dhawan Space Centre in Sriharikota\",\n",
    "    \"Chandrayaan-3 was launched earlier in the year 2023\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_embeddings_list: List[SparseEmbedding] = list(\n",
    "    model.embed(documents, batch_size=6)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseEmbedding(values=array([0.0529714 , 0.0196357 , 0.36459896, 1.38508725, 0.71776628,\n",
       "       0.12668033, 0.46230948, 0.4467729 , 0.26897642, 1.01519895,\n",
       "       1.565534  , 0.29412517, 1.53102434, 0.59785426, 1.10018313,\n",
       "       0.02079809, 0.09955856, 0.44249222, 0.09747887, 1.53519821,\n",
       "       1.36765611, 0.1574067 , 0.49882376, 0.38629326, 0.7661289 ,\n",
       "       1.25804996, 0.39058331, 0.27236342, 0.45152119, 0.48261827,\n",
       "       0.26084986, 1.35912728, 0.70710504, 1.71639669]), indices=array([ 1010,  1011,  1016,  1017,  2001,  2018,  2034,  2093,  2117,\n",
       "        2319,  2353,  2509,  2634,  2686,  2796,  2817,  2922,  2959,\n",
       "        3003,  3148,  3260,  3390,  3462,  3523,  3822,  4231,  4316,\n",
       "        4774,  5590,  5871,  6416, 11926, 12076, 16469]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "sparse_embeddings_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token at index 1010 has weight 0.05297140032052994\n",
      "Token at index 1011 has weight 0.019635701552033424\n",
      "Token at index 1016 has weight 0.36459895968437195\n",
      "Token at index 1017 has weight 1.385087251663208\n",
      "Token at index 2001 has weight 0.717766284942627\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Token at index {sparse_embeddings_list[0].indices[i]} has weight {sparse_embeddings_list[0].values[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(SparseTextEmbedding.list_supported_models()[0][\"sources\"][\"hf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chandra\": 1.716396689414978,\n",
      "    \"third\": 1.565533995628357,\n",
      "    \"##ya\": 1.5351982116699219,\n",
      "    \"india\": 1.5310243368148804,\n",
      "    \"3\": 1.385087251663208,\n",
      "    \"mission\": 1.3676561117172241,\n",
      "    \"lunar\": 1.3591272830963135,\n",
      "    \"moon\": 1.2580499649047852,\n",
      "    \"indian\": 1.100183129310608,\n",
      "    \"##an\": 1.0151989459991455,\n",
      "    \"3rd\": 0.7661288976669312,\n",
      "    \"was\": 0.717766284942627,\n",
      "    \"spacecraft\": 0.7071050405502319,\n",
      "    \"space\": 0.5978542566299438,\n",
      "    \"flight\": 0.49882376194000244,\n",
      "    \"satellite\": 0.4826182723045349,\n",
      "    \"first\": 0.46230947971343994,\n",
      "    \"expedition\": 0.4515211880207062,\n",
      "    \"three\": 0.44677290320396423,\n",
      "    \"fourth\": 0.44249221682548523,\n",
      "    \"vehicle\": 0.3905833065509796,\n",
      "    \"iii\": 0.3862932622432709,\n",
      "    \"2\": 0.36459895968437195,\n",
      "    \"##3\": 0.29412516951560974,\n",
      "    \"planet\": 0.27236342430114746,\n",
      "    \"second\": 0.2689764201641083,\n",
      "    \"missions\": 0.26084986329078674,\n",
      "    \"launched\": 0.15740670263767242,\n",
      "    \"had\": 0.12668032944202423,\n",
      "    \"largest\": 0.09955856204032898,\n",
      "    \"leader\": 0.09747886657714844,\n",
      "    \",\": 0.05297140032052994,\n",
      "    \"study\": 0.02079809457063675,\n",
      "    \"-\": 0.019635701552033424\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def get_tokens_and_weights(sparse_embedding, tokenizer):\n",
    "    token_weight_dict = {}\n",
    "    for i in range(len(sparse_embedding.indices)):\n",
    "        token = tokenizer.decode([sparse_embedding.indices[i]])\n",
    "        weight = sparse_embedding.values[i]\n",
    "        token_weight_dict[token] = weight\n",
    "\n",
    "    # Sort the dictionary by weights\n",
    "    token_weight_dict = dict(sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    return token_weight_dict\n",
    "\n",
    "# Test the function with the first SparseEmbedding\n",
    "print(json.dumps(get_tokens_and_weights(sparse_embeddings_list[index], tokenizer), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
